{
 "metadata": {
  "name": "",
  "signature": "sha256:0bdcb7387075aa8f23874397aff32392943dab0845dd26ed1b90c2675381bb07"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction to <tt>RooStats</tt> for doing statistical data interpretation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Outline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<ol>\n",
      "<li>Introduction</li>\n",
      "<li>Test Statistics</li>\n",
      "<li>Significances</li>\n",
      "<li>Upper Limits</li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The thorough interpretation of a measurement is often a delicate task. Especially questions like the following require advanced statistical methods in order to arrive at reliable statements/conclusions.\n",
      "<ul>\n",
      "<li>How significant is my (signal) observation?</li>\n",
      "<li>I know the best fit value for my parameter, but how large is its uncertainty?</li>\n",
      "<li>I haven't seen the expected signal. What can I learn from this?</li>\n",
      "</ul><br />\n",
      "In this tutorial these questions are discussed and it is demonstrated how to use the <a href=\"https://twiki.cern.ch/twiki/bin/view/RooStats/WebHome\"><tt>RooStats</tt> framework</a> to perform statistical data interpretation. A very good discussion of the different test statistics and their approximations is given in <a href=\"http://arxiv.org/abs/1007.1727\">Asymptotic formulae for likelihood-based tests of new physics [arxiv:1007.1727]</a>."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Frequentist vs. Bayesian Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two major schools of statistics: <i>frequentist</i> and <i>bayesian</i>. In many cases, the results obtained with both methods are surprisingly similiar. Nevertheless, the meaning of frequentist and bayesian results is based on different philosophical concepts. Here, the main differences are highlighted briefly. This discussion is merely meant to raise your awareness for differences rather than to provide a full, comprehensive comparison.<br />\n",
      "A long story short: <i><b>Bayesians</b></i> treat the <b>parameter value as random variable</b> and keep boudaries fixed while <i><b>Frequentist</b></i> keep the <b>unknown true value of the parameter fixed</b> and regard the interval boundaries as random variables. Assuming a scientific paper reports a measurement of a parameter as $x = x_0 \\pm \\Delta x$ (CL = 68.3%).<br />\n",
      "<ul>\n",
      "<li><i>Bayesian interpretation:</i> The probability of the true value of $x$ being inside the interval $[x_0 - \\Delta x,x_0 + \\Delta x ]$ is 68.3% This is called a <u>credibility</u> interval.</li>\n",
      "<li><i>Frequentist interpretation:</i> Suppose you would re-run the measurement many times and for each run you construct an interval. Then, the true value of $x$ would be inside the interval in 68.3% of the cases. This is called a <u>confidence</u> interval.</li>\n",
      "</ul><br />\n",
      "In Bayesian statistics one makes a statement about the <i>true</i> value of the parameter being inside a given interval with a certain <u>credibility</u>. A frequentist interval can only be interpreted as part of an ensemble of intervals out of which a certain fraction will <u>cover</u> the true value of the parameter. Due to personal prejudice, this tutorial is focused on frequentist methods. Nevertheless, it should be emphasised that <tt>RooStats</tt> is equally well-suited to perform bayesian data analysis."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Toy Model and parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the following tasks we need a toy model which we will take to be an exponentially decaying background and a Gaussian-shaped signal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ROOT\n",
      "import rootnotes   # for displaying ROOT figure inline\n",
      "import rootprint   # for capturing ROOT output\n",
      "c1 = rootnotes.default_canvas()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ws = ROOT.RooWorkspace(\"ws\")\n",
      "ws.factory(\"Exponential:e(x[0,500],tau[-0.01,-5,-0.001])\")       # exponential background PDF\n",
      "ws.factory(\"Gaussian::g(x,mean[250],sigma[15])\")                 # Gaussian signal PDF\n",
      "ws.factory(\"SUM::model(b[1000,0,5000]*e,s[10,0,100]*g)\")         # composite model\n",
      "ws.saveSnapshot(\"initial_pars\",\"tau,mean,sigma,s,b\")             # make snapshot of parameter values\n",
      "ws.var(\"s\").setVal(0)\n",
      "ws.saveSnapshot(\"b_only\",\"tau,mean,sigma,s,b\")                   # make snapshot for background-only hypothesis\n",
      "ws.var(\"s\").setVal(10)\n",
      "ws.saveSnapshot(\"signal\",\"tau,mean,sigma,s,b\")                   # make snapshot for background+signal hypothesis\n",
      "ws.var(\"s\").setVal(30)\n",
      "ws.saveSnapshot(\"more_signal\",\"tau,mean,sigma,s,b\")              # make snapshot for enhanced signal hypothesis\n",
      "%rp ws.Print()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This toy model depends on the variables $x,\\tau,\\mu,\\sigma,s,b$. The results of our measurement is $x$ and the parameter we are interested in is the number of observed signal events $s$. Therefore, the variables are categorised as follows:\n",
      "<ul>\n",
      "<li>the <i>observable</i> $x$,</li>\n",
      "<li>the <i>nuisance parameters</i> $\\tau, \\mu, \\sigma$ and $b$,</li>\n",
      "<li>and the <i>parameter of interest</i> $s$.</li>\n",
      "</ul><br />\n",
      "The likelihood function of the toy model is given by\n",
      "$$\\mathcal{L}(\\vec{x}|s,\\vec{\\theta}) = \\prod_{i=1}^n \\left[ s \\times \\text{G}(x_i|\\mu,\\sigma) + b \\times \\text{Exp}(x_i|\\tau) \\right] \\times \\text{Pois}(n|s+b)$$ where $\\vec{x}$ is the set of observations of $x$ and $\\vec{\\theta}$ denotes the set of nuisance parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ws.loadSnapshot(\"initial_pars\")\n",
      "x = ws.var(\"x\")\n",
      "s = ws.var(\"s\")\n",
      "pdf = ws.pdf(\"model\")\n",
      "f = x.frame()\n",
      "d_obs = pdf.generate(ROOT.RooArgSet(x))  # generate some data which we will use as observation\n",
      "pdf.fitTo(d_obs)\n",
      "d_obs.plotOn(f)\n",
      "pdf.plotOn(f)\n",
      "f.Draw()\n",
      "c1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Significances and $p$-values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Much of the following is related to the field of <i>hypothesis testing</i> in which $p$-values play an important role. As a reminder, the definition of a $p$-value shall be repeated here:<br /><br />\n",
      "<i>\"Given some data $\\vec{x}$ and a hypothesis $H$, the $p$-value is defined as the probability, under the assumption of $H$ being true, of obtaining data with equal or worse compatibility than the actual observation.\"</i><br /><br />\n",
      "The following example illustrates this concept for a normal distributed observable for two hypothesis with different means and widths."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import norm\n",
      "import numpy as np\n",
      "x = np.linspace(-6,6,1000)\n",
      "g1 = norm(loc=0,scale=1)\n",
      "g2 = norm(loc=3,scale=2)\n",
      "y1 = g1.pdf(x)\n",
      "y2 = g2.pdf(x)\n",
      "plt.plot(x,y1)\n",
      "plt.plot(x,y2)\n",
      "plt.plot([1.5,1.5],[0,0.3])\n",
      "plt.gca().fill_between(x,0,y1,where=x>1.5)\n",
      "plt.gca().fill_between(x,0,y2,where=x<1.5,color=\"green\")\n",
      "plt.text(2,0.1,\"p-value\",color=\"blue\",size=\"x-large\")\n",
      "plt.text(-1,0.15,\"p-value\",color=\"green\",size=\"x-large\")\n",
      "plt.text(1.6,0.25,\"x = 1.5\",color=\"red\",size=\"x-large\")\n",
      "plt.text(-4,0.35,\"H = G(0,1)\",color=\"blue\",size=\"x-large\")\n",
      "plt.text(-4,0.32,\"p = %.1f%%\" % (100*g1.sf(1.5)),color=\"blue\",size=\"x-large\")\n",
      "plt.text(2,0.35,\"H = G(3,2)\",color=\"green\",size=\"x-large\")\n",
      "plt.text(2,0.32,\"p = %.1f%%\" % (100*g2.cdf(1.5)),color=\"green\",size=\"x-large\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In everyday problems it is less obvious how <i>worse compatibility</i> is defined. In order to facilitate the comparison of two datasets with respect to their compatibility with a certain hypothesis, one needs to make use of so-called <i>test statistics</i>.<br />\n",
      "Papers usually cite significances rather than $p$-values as the latter can become very tiny. The significance is defined as the value $z$ such that the right-tail probability of a normal distribution above $z$ is equal to $p$.\n",
      "$$p = \\int_z^{+\\infty} \\text{G}(x,0,1) dx \\quad \\Rightarrow \\quad z = \\Phi^{-1}(1 - p)$$\n",
      "wher $\\Phi^{-1}$ is the inverse of the cumulative normal distribution. Significances and $p$-values can easily be converted using <br />\n",
      "<code>p = ROOT.Math.normal_cdf_c(z,1)</code><br />\n",
      "<code>z = ROOT.Math.normal_quantile_c(p,1)</code>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Test Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Measurements can be difficult to describe in a general approach as they can be very different (e.g. varying number of observables, discrete vs. continue observables etc). Therefore, the statistical data interpretation happens on a higher, more abstract level (well, this \"abstract\" level is going to be $\\mathbb{R}$ $\\dots$). You can think of a <i>test statistic</i> $t$ as a mapping from your space of observables to this abstract level: $t: \\text{measurement} \\mapsto \\mathbb{R}$. A test statistic should have the following properties:\n",
      "<ul>\n",
      "<li>It should depend on the hypothesis (e.g. the signal strength, position of the Gaussian etc).</li>\n",
      "<li>It should depend on the data observed.</li>\n",
      "<li>It should allow for a quantitative evaluation of the compatibility of the data with the given hypothesis.</li>\n",
      "</ul><br />\n",
      "A common (and very reasonable) choice is the profile likelihood ratio $\\lambda_s = \\frac{\\mathcal{L}(\\vec{x}|s,\\hat{\\hat{\\theta}})}{\\mathcal{L}(\\vec{x}|\\hat{s},\\hat{\\theta})}$ where $s$ is the parameter of interest and $\\theta$ is the set of nuisance parameters. Hereby, $\\hat{s}$ and $\\hat{\\theta}$ are the unconditional maximum likelihood estimators (MLE) which maximise the likelihood function globally. On the contrary, $\\hat{\\hat{\\theta}}$ is the conditional MLE which maximises the likelihood function for a given (and fixed) value of $s$. It is wortwhile to note that $\\lambda_s$ is for a given dataset $\\vec{x}$ only a function of $s$ and $0 \\le \\lambda_s \\le 1$. Values close to 1 correspond to good compatibility between the tested value of $s$ and the data while values close to 0 indicate a very poor agreement. The test statistic is then defined as\n",
      "$$t_s = -2 \\log \\lambda_s = -2 \\log \\frac{\\mathcal{L}(\\vec{x}|s,\\hat{\\hat{\\theta}})}{\\mathcal{L}(\\vec{x}|\\hat{s},\\hat{\\theta})} = 2 \\times \\left[ - \\log \\mathcal{L}(\\vec{x}|s,\\hat{\\hat{\\theta}}) - (- \\log \\mathcal{L}(\\vec{x}|\\hat{s},\\hat{\\theta})) \\right]\\ .$$\n",
      "The test statistics $t_s$ takes on values between 0 and $+\\infty$ with increasing values representing decreasing compatibility between the data and the tested hypothesis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib import cm\n",
      "x = np.arange(-3,3,0.1)\n",
      "y = np.arange(-3,3,0.1)\n",
      "xx,yy = np.meshgrid(x,y)\n",
      "z = 2*xx**2 + yy**2\n",
      "ax = plt.gca(projection='3d')\n",
      "ax.plot_surface(xx,yy,z,cmap=cm.gist_rainbow,rstride=1,cstride=1,linewidth=0)\n",
      "ax.plot(xs=len(y)*[-1.5],ys=y,zs=2*1.5**2 + y**2,color=\"red\",linewidth=3)\n",
      "ax.set_xlabel(\"x\")\n",
      "ax.set_ylabel(\"y\")\n",
      "ax.set_zlabel(\"L(x,y)\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Using sampling distributions to calculate $p$-values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After having chosen a test statistics, one can evaluate it for the hypothesis of interest on the measured dataset which yields $t_{s,\\mathrm{obs}}$ (= a single number). In order to interpret this single number (i.e. calculate a $p$-value) one needs to know the distribution of $t_s$. The so-called <i>sampling distribution</i> $f(t_s|s^\\prime)$ gives the probability of observing a value of the test statistic $t_s$ given that the true value of $s$ is equal to $s^\\prime$. In general, the tested hypothesis $s$ does not necessarily need to be the same as the assumed true hypothesis $s^\\prime$.<br />\n",
      "The code below generates the two sampling distributions $f(t_{s=10}|s^\\prime = 10)$ and $f(t_{s=0}|s^\\prime = 10)$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h_t0 = ROOT.TH1F(\"h_t0\",\"h_t0;t_{s};f(t_{s}|s')\",50,0,25)\n",
      "h_t10 = ROOT.TH1F(\"h_t10\",\"sampling distribution;t_{s};f(t_{s}|s^{#prime})\",50,0,25)\n",
      "ntoys = 2000\n",
      "x = ws.var(\"x\")\n",
      "s = ws.var(\"s\")\n",
      "pdf = ws.pdf(\"model\")\n",
      "for i in range(0,ntoys):\n",
      "    if i % int(0.01 * ntoys) == 0:\n",
      "        print \"\\r%.0f%% done\" % (i*100./ntoys),\n",
      "    ws.loadSnapshot(\"signal\")\n",
      "    toy_data = pdf.generate(ROOT.RooArgSet(x))\n",
      "    r_uncond = pdf.fitTo(toy_data,ROOT.RooFit.Save(),ROOT.RooFit.PrintLevel(-1))\n",
      "    s.setConstant(True)\n",
      "    s.setVal(10)\n",
      "    r_cond10 = pdf.fitTo(toy_data,ROOT.RooFit.Save(),ROOT.RooFit.PrintLevel(-1))\n",
      "    s.setVal(0)\n",
      "    r_cond0 = pdf.fitTo(toy_data,ROOT.RooFit.Save(),ROOT.RooFit.PrintLevel(-1))\n",
      "    h_t0.Fill(2*(r_cond0.minNll() - r_uncond.minNll()))\n",
      "    h_t10.Fill(2*(r_cond10.minNll() - r_uncond.minNll()))\n",
      "    del toy_data\n",
      "    del r_uncond\n",
      "    del r_cond10\n",
      "    del r_cond0\n",
      "h_t10.Draw()\n",
      "h_t0.SetLineColor(ROOT.kRed)\n",
      "h_t0.Draw(\"same\")\n",
      "c1.SetLogy(1)\n",
      "l = ROOT.TLegend(0.6,0.6,0.85,0.85)\n",
      "l.SetFillColor(ROOT.kWhite)\n",
      "l.AddEntry(h_t10,\"f(t_{s=10}|s' = 10)\")\n",
      "l.AddEntry(h_t0,\"f(t_{s=0}|s' = 10)\")\n",
      "l.Draw(\"same\")\n",
      "c1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since larger values for $t_s$ indicate a worse agreement, the $p$-value can be calculated from the sampling distribution as\n",
      "$$p = \\int_{t_{s,obs}}^{+\\infty} f(t_s|s) d t_s$$\n",
      "Note: here $s = s^\\prime$ as the $p$-value is defined under the assumption that the tested hypothesis is true."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Significances"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3.1 Do it yourself"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to state the significance of a measurement (e.g. the observation of some signal), one needs to specify the $p$-value for the null hypothesis (usually a background-only model). The smaller the $p$-value (= the larger the significance) the more unlikely the data can be explained the by the null hypothesis. A large significance <b>NEVER EVER</b> means that your signal has been discovered. It only means that the data can not be described by the null hypothesis. Due to this reason, an observed significance only depends on the data and the null hypothesis and never on the signal hypothesis. On the contrary, the expected significance, of course, depends on your signal model.<br />\n",
      "The test statistic for calculating significances is given by:\n",
      "$$q_0 = \\left\\{ \\begin{matrix} -2 \\log \\frac{\\mathcal{L}(\\vec{x}|s,\\hat{\\hat{\\theta}})}{\\mathcal{L}(\\vec{x}|\\hat{s},\\hat{\\theta})}, & \\text{for } \\hat{s} \\ge 0 \\\\ 0, & \\text{for } \\hat{s} < 0 \\end{matrix} \\right. $$\n",
      "This choice only considers an <i>excess</i> over the background model as indication for a signal while a deficit in data will not be regarded as hint of new phyiscs. This is appropriate in most situations but may fail in certain special cases (e.g. disappearance in neutrino oscillations).<br />\n",
      "The procedure for calculating the observed significance is the following:\n",
      "<ol>\n",
      "<li>Generate the sampling distribution $f(q_0|s'=0)$.</li>\n",
      "<li>Calculate $q_{0,\\mathrm{obs}}$ for the measured dataset.</li>\n",
      "<li>Calculate the observed $p$-value by integrating the sampling distribution.</li>\n",
      "</ol>\n",
      "In order to estimate the expected discovery significance one needs to replace $q_{0,\\mathrm{obs}}$ by the <i>expected</i> observation of $q_0$ under the assumption that the signal is present. This can be calculated as the mean of the sampling distribution $f(q_0|s'=10)$ (for the example of expecting 10 signal events). It is clear that the expected discovery significance depends on the signal hypothesis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate the q_0 test statistic as defined above\n",
      "# here it is assumed that the background-only hypothesis corresponds to POI=0\n",
      "def q0(data,pdf,poi):\n",
      "    q_0 = -1\n",
      "    # make sure the POI can become negative\n",
      "    if not poi.getMin() < 0:\n",
      "        poi.setMin(-1)\n",
      "    poi.setConstant(0)\n",
      "    r_uncond = pdf.fitTo(data,ROOT.RooFit.Save())\n",
      "    if poi.getVal() < 0:\n",
      "        q_0 = 0.0\n",
      "    else:\n",
      "        poi.setVal(0)\n",
      "        poi.setConstant(1)\n",
      "        r_cond = pdf.fitTo(data,ROOT.RooFit.Save())\n",
      "        q_0 = 2*(r_cond.minNll() - r_uncond.minNll())\n",
      "        del r_cond\n",
      "    del r_uncond\n",
      "    return q_0\n",
      "\n",
      "# generate sampling distribution\n",
      "h_q0_0 = ROOT.TH1F(\"h_q0_0\",\"h_q0_0\",50,0,25)\n",
      "h_q0_10 = ROOT.TH1F(\"h_q0_10\",\"h_q0_10\",50,0,25)\n",
      "h_q0_30 = ROOT.TH1F(\"h_q0_30\",\"h_q0_30\",50,0,25)\n",
      "ntoys = 3000\n",
      "s.setMin(-1)\n",
      "for i in range(0,ntoys):\n",
      "    if i % 10 == 0:\n",
      "        print \"\\r%.0f%% done\" % (i*100./ntoys),\n",
      "    # background-only\n",
      "    ws.loadSnapshot(\"b_only\")\n",
      "    d = pdf.generate(ROOT.RooArgSet(x))\n",
      "    h_q0_0.Fill(q0(d,pdf,s))\n",
      "    del d\n",
      "    # background+signal\n",
      "    ws.loadSnapshot(\"signal\")\n",
      "    d = pdf.generate(ROOT.RooArgSet(x))\n",
      "    h_q0_10.Fill(q0(d,pdf,s))\n",
      "    del d\n",
      "    # enhanced signal\n",
      "    ws.loadSnapshot(\"more_signal\")\n",
      "    d = pdf.generate(ROOT.RooArgSet(x))\n",
      "    h_q0_30.Fill(q0(d,pdf,s))\n",
      "    del d\n",
      "\n",
      "# calculate t_{s=0,obs}\n",
      "q0_obs = q0(d_obs,pdf,s)\n",
      "q0_10_exp = h_q0_10.GetMean()\n",
      "q0_30_exp = h_q0_30.GetMean()\n",
      "\n",
      "# plot everything\n",
      "h_q0_0.Scale(1./h_q0_0.Integral(\"width\"))\n",
      "h_q0_10.Scale(1./h_q0_10.Integral(\"width\"))\n",
      "h_q0_30.Scale(1./h_q0_30.Integral(\"width\"))\n",
      "\n",
      "h_q0_10.SetLineColor(ROOT.kRed)\n",
      "h_q0_30.SetLineColor(ROOT.kGreen+3)\n",
      "\n",
      "h_q0_0.Draw()\n",
      "h_q0_10.Draw(\"same\")\n",
      "h_q0_30.Draw(\"same\")\n",
      "\n",
      "# plot chi2 function\n",
      "f_chi2 = ROOT.TF1(\"chi2\",\"0.5*ROOT::Math::chisquared_pdf(x,1)\",0,25)\n",
      "f_chi2.SetLineColor(ROOT.kGreen+3)\n",
      "f_chi2.Draw(\"same\")\n",
      "\n",
      "# plot observed/expected test statistics\n",
      "l1 = ROOT.TLine(q0_obs,0.001,q0_obs,1)\n",
      "l2 = ROOT.TLine(q0_10_exp,0.001,q0_10_exp,1)\n",
      "l3 = ROOT.TLine(q0_30_exp,0.001,q0_30_exp,1)\n",
      "\n",
      "l2.SetLineColor(ROOT.kRed)\n",
      "l3.SetLineColor(ROOT.kGreen+3)\n",
      "\n",
      "l1.Draw(\"same\")\n",
      "l2.Draw(\"same\")\n",
      "l3.Draw(\"same\")\n",
      "\n",
      "c1.SetLogy(1)\n",
      "c1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above test statistic is a smart choice as their asymptotic distribution is well-known. If certain requirements (e.g. enough statistics, not near parameter boundardies, no pathologies) are fulfilled, the sampling distribution is given by\n",
      "$$f(q_0|s'=0) = \\frac{1}{2}\\chi^2(q_0,\\text{ndf})$$\n",
      "where the number of degrees of freedom (ndf) is equal to the number of parameters of interest. In this case, the significance can easily be calculated as $z = \\sqrt{q_{0,\\mathrm{obs}}}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# helper function to integrate tail of sampling distribution\n",
      "def integrate_sampling_dist(ts_obs,sampling_dist):\n",
      "    # integrate sampling distribution\n",
      "    bin = sampling_dist.GetXaxis().FindFixBin(ts_obs)\n",
      "    p = sum(map(sampling_dist.GetBinContent,range(bin+1,sampling_dist.GetNbinsX()+2)))*sampling_dist.GetBinWidth(1)\n",
      "    p += (ts_obs - sampling_dist.GetBinLowEdge(bin))*sampling_dist.GetBinContent(bin)\n",
      "    return p\n",
      "\n",
      "from math import sqrt\n",
      "p0 = integrate_sampling_dist(q0_obs,h_q0_0)\n",
      "p10 = integrate_sampling_dist(q0_10_exp,h_q0_0)\n",
      "p30 = integrate_sampling_dist(q0_30_exp,h_q0_0)\n",
      "print \"observed significance Z = %.3f/%.3f (toys,asymptotic)\" % (ROOT.Math.normal_quantile_c(p0,1),sqrt(q0_obs))\n",
      "print \"expected significance Z(s=10) = %.3f/%.3f (toys,asymptotic)\" % (ROOT.Math.normal_quantile_c(p10,1),sqrt(q0_10_exp))\n",
      "print \"expected significance Z(s=30) = %.3f/%.3f (toys,asymptotic)\" % (ROOT.Math.normal_quantile_c(p30,1),sqrt(q0_30_exp))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3.2 With <tt>RooStats</tt>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The calculation of significances with <tt>RooStats</tt> requires the definition of hypotheses in terms of <tt>ModelConfig</tt> objects. Each <tt>ModelConfig</tt> needs to contain a <i>snapshot</i> which is the parameter point corresponding to the hypothesis (here, they contain different values for the number of signal events $s$)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = ws.var(\"s\")\n",
      "s.setMin(0)\n",
      "# restore all parameter values\n",
      "ws.loadSnapshot(\"initial_pars\")\n",
      "\n",
      "# configure null hypothesis\n",
      "null = ROOT.RooStats.ModelConfig(\"null\",ws)\n",
      "null.SetPdf(\"model\") \n",
      "null.SetObservables(\"x\")\n",
      "null.SetNuisanceParameters(\"tau,mean,sigma,b\")\n",
      "null.SetParametersOfInterest(\"s\")\n",
      "s.setVal(0)\n",
      "null.SetSnapshot(null.GetParametersOfInterest())\n",
      "\n",
      "# configure alternative hypothesis\n",
      "alt = ROOT.RooStats.ModelConfig(\"alt\",ws)\n",
      "alt.SetPdf(\"model\") \n",
      "alt.SetObservables(\"x\")\n",
      "alt.SetNuisanceParameters(\"tau,mean,sigma,b\")\n",
      "alt.SetParametersOfInterest(\"s\")\n",
      "s.setVal(10)\n",
      "alt.SetSnapshot(alt.GetParametersOfInterest())\n",
      "\n",
      "%rp null.Print()\n",
      "%rp alt.Print()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculating the observed significance using toys with <tt>RooStats</tt> is straightforward. Unfortunately, I am not aware of a possibility for calculating the <i>expected</i> significance using toys in <tt>RooStats</tt>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialise calculator with data and ModelConfig objects\n",
      "fc = ROOT.RooStats.FrequentistCalculator(d_obs,alt,null)\n",
      "# we only need toys for the null hypothesis to get the observed significance\n",
      "# note: order of null and alternate hypothesis is switched wrt constructor\n",
      "fc.SetToys(ntoys,0)\n",
      "# initialise one-sided (i.e. q0 = 0 for hat(s) < 0) profile likelihood test statistic\n",
      "profll = ROOT.RooStats.ProfileLikelihoodTestStat(null.GetPdf())\n",
      "profll.SetOneSidedDiscovery(True)\n",
      "\n",
      "# configure the sampler of the test statistic\n",
      "toymc = fc.GetTestStatSampler()\n",
      "toymc.SetTestStatistic(profll)\n",
      "\n",
      "if not null.GetPdf().canBeExtended():\n",
      "    toymc.SetNEventsPerToy(1)\n",
      "    \n",
      "ht = fc.GetHypoTest()\n",
      "print \"significance Z = %.3f (RooStats::AsymptoticCalculator)\" % ht.Significance()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculating the observed significance using the asymptotic approximation is even simpler."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# initialise asymptotic calculator\n",
      "ac = ROOT.RooStats.AsymptoticCalculator(d_obs,alt,null)\n",
      "ac.SetOneSidedDiscovery(True)\n",
      "%rp ht = ac.GetHypoTest()\n",
      "print \"significance Z = %.3f (RooStats::AsymptoticCalculator)\" % ht.Significance() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The approximative expected discovery significance can be calculated using the <i>Asimov</i> data set. The <i>Asimov</i> data set is the hypothetical observation for the given parameter point without any statistical fluctuations. It can be generated using the static helper methods from the <code>AsymptoticCalculator</code> class. When generating the Asimov data, one has some freedom in the choice of the values for the nuisance parameters. There are two possible options:\n",
      "<ol>\n",
      "<li>Pick fixed valuea for the nuisance parameters (e.g. values from theory, derived from other measurements, etc).</li>\n",
      "<li>In case one has actual data at hand, fit the nuisance parameters to this data and use the post-fit values to generate the Asimov data set.</li>\n",
      "</ol>\n",
      "Clearly, option 2 is only available if one already has some measurement and, for instance, on wants to compare observed and expected significances. To estimate the sensitive of future experiments, option 1 is the only way to go.<br />\n",
      "The standard procedure in ATLAS is option 2. One should note that this approach can sometimes have surprising effects: a deficit in your observed data will improve the expected significance. This is due to the fact the nuisance parameters fitted to the data observed tend to reduce the background. Therefore, the Asimov data set will have a better signal-to-background ratio than your nominal expectation. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ws.loadSnapshot(\"more_signal\")\n",
      "# dummy dataset for global observables\n",
      "globObs = ROOT.RooArgSet()\n",
      "# generate Asimov data\n",
      "asimov = ROOT.RooStats.AsymptoticCalculator.MakeAsimovData(alt,alt.GetPdf().getVariables(),globObs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The generated Asimov data set is exactly the expectation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pdf.fitTo(asimov)\n",
      "f = x.frame()\n",
      "asimov.plotOn(f,ROOT.RooFit.DataError(ROOT.RooAbsData.Poisson))\n",
      "pdf.plotOn(f)\n",
      "f.Draw()\n",
      "c1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the same as above but now with Asimov data set\n",
      "ac = ROOT.RooStats.AsymptoticCalculator(asimov,alt,null)\n",
      "ac.SetOneSidedDiscovery(True)\n",
      "%rp ht = ac.GetHypoTest()\n",
      "print \"expected significance Z(s=30) = %.3f (RooStats::AsymptoticCalculator)\" % ht.Significance() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Intervals and Upper Limits"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The calculation of confidence intervals and upper limits using toys in <tt>Roostats</tt> is a lengthy task. The following two tutorials may serve as good starting points:<br />\n",
      "<a href=\"https://root.cern.ch/root/htmldoc/tutorials/roostats/OneSidedFrequentistUpperLimitWithBands.C.html\">OneSidedFrequentistUpperLimitWithBands.C</a><br />\n",
      "<a href=\"https://root.cern.ch/root/htmldoc/tutorials/roostats/StandardHypoTestInvDemo.C.html\">StandardHypoTestInvDemo.C</a><br />\n",
      "Conceptually, an upper limit is the identical to an one-sided confidence interval and thus, the principles for calculating intervals can be applied. The confidence interval in $s$ for a given confidence level $\\alpha$ is defined as the set of all points $s$ for which the corresponding $p$-value is not smaller than $1 - \\alpha$. That means, a point $s$ is inside the confidence interval if\n",
      "$$p_s = \\int_{t_{s,\\mathrm{obs}}}^{+\\infty} f(t_s|s) d t_s \\ge 1 - \\alpha \\ .$$\n",
      "In practice, confidence intervals are usually connected and it is sufficient to find the two endpoints of the interval. Nevertheless, this task requires the generation of many sampling distributions (which in turn generate many toy datasets) and is therefore a CPU intensive task.<br />\n",
      "However, simple intervals based on the likelihood principle (which are exact in the asymptotic regime) can be calculated easily. In this case, the endpoints of the interval are given by the values of $s$ for which $t_s = \\chi^{-1}(1 - \\alpha)$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plc = ROOT.RooStats.ProfileLikelihoodCalculator(d_obs,alt)\n",
      "plc.SetConfidenceLevel(0.683)\n",
      "i = plc.GetInterval()\n",
      "print \"68.3%% confidence interval on s = [%.2f,%.2f]\" % (i.LowerLimit(s),i.UpperLimit(s))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sframe = s.frame(ROOT.RooFit.Range(0,50))\n",
      "nll = pdf.createNLL(d_obs)\n",
      "# create profile\n",
      "# important: this profile is missing a factor 2 of our definition of the test statistic\n",
      "profile = nll.createProfile(ROOT.RooArgSet(s))\n",
      "profile.plotOn(sframe,ROOT.RooFit.Name(\"profile\"))\n",
      "nll.plotOn(sframe,ROOT.RooFit.LineColor(ROOT.kRed),ROOT.RooFit.LineStyle(ROOT.kDashed),ROOT.RooFit.ShiftToZero(),ROOT.RooFit.Name(\"nll\"))\n",
      "sframe.SetMinimum(0)\n",
      "sframe.SetMaximum(10)\n",
      "sframe.Draw()\n",
      "# dummy factors of 0.5 due to different normalisation of test statistic\n",
      "l = ROOT.TLine(0,0.5*ROOT.Math.chisquared_quantile(0.683,1),50,0.5*ROOT.Math.chisquared_quantile(0.683,1))\n",
      "l2 = ROOT.TLine(0,0.5*ROOT.Math.chisquared_quantile(0.95,1),50,0.5*ROOT.Math.chisquared_quantile(0.95,1))\n",
      "l.Draw(\"same\")\n",
      "l2.Draw(\"same\")\n",
      "c1.SetLogy(0)\n",
      "c1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}