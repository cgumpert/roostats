{
 "metadata": {
  "name": "",
  "signature": "sha256:0bdcb7387075aa8f23874397aff32392943dab0845dd26ed1b90c2675381bb07"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction to <tt>RooStats</tt> for doing statistical data interpretation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Outline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<ol>\n",
      "<li>Introduction</li>\n",
      "<li>Test Statistics</li>\n",
      "<li>Significances</li>\n",
      "<li>Upper Limits</li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The thorough interpretation of a measurement is often a delicate task. Especially questions like the following require advanced statistical methods in order to arrive at reliable statements/conclusions.\n",
      "<ul>\n",
      "<li>How significant is my (signal) observation?</li>\n",
      "<li>I know the best fit value for my parameter, but how large is its uncertainty?</li>\n",
      "<li>I haven't seen the expected signal. What can I learn from this?</li>\n",
      "</ul><br />\n",
      "In this tutorial these questions are discussed and it is demonstrated how to use the <a href=\"https://twiki.cern.ch/twiki/bin/view/RooStats/WebHome\"><tt>RooStats</tt> framework</a> to perform statistical data interpretation. A very good discussion of the different test statistics and their approximations is given in <a href=\"http://arxiv.org/abs/1007.1727\">Asymptotic formulae for likelihood-based tests of new physics [arxiv:1007.1727]</a>."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Frequentist vs. Bayesian Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are two major schools of statistics: <i>frequentist</i> and <i>bayesian</i>. In many cases, the results obtained with both methods are surprisingly similiar. Nevertheless, the meaning of frequentist and bayesian results is based on different philosophical concepts. Here, the main differences are highlighted briefly. This discussion is merely meant to raise your awareness for differences rather than to provide a full, comprehensive comparison.<br />\n",
      "A long story short: <i><b>Bayesians</b></i> treat the <b>parameter value as random variable</b> and keep boudaries fixed while <i><b>Frequentist</b></i> keep the <b>unknown true value of the parameter fixed</b> and regard the interval boundaries as random variables. Assuming a scientific paper reports a measurement of a parameter as $x = x_0 \\pm \\Delta x$ (CL = 68.3%).<br />\n",
      "<ul>\n",
      "<li><i>Bayesian interpretation:</i> The probability of the true value of $x$ being inside the interval $[x_0 - \\Delta x,x_0 + \\Delta x ]$ is 68.3% This is called a <u>credibility</u> interval.</li>\n",
      "<li><i>Frequentist interpretation:</i> Suppose you would re-run the measurement many times and for each run you construct an interval. Then, the true value of $x$ would be inside the interval in 68.3% of the cases. This is called a <u>confidence</u> interval.</li>\n",
      "</ul><br />\n",
      "In Bayesian statistics one makes a statement about the <i>true</i> value of the parameter being inside a given interval with a certain <u>credibility</u>. A frequentist interval can only be interpreted as part of an ensemble of intervals out of which a certain fraction will <u>cover</u> the true value of the parameter. Due to personal prejudice, this tutorial is focused on frequentist methods. Nevertheless, it should be emphasised that <tt>RooStats</tt> is equally well-suited to perform bayesian data analysis."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Toy Model and parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the following tasks we need a toy model which we will take to be an exponentially decaying background and a Gaussian-shaped signal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ROOT\n",
      "import rootnotes   # for displaying ROOT figure inline\n",
      "import rootprint   # for capturing ROOT output\n",
      "c1 = rootnotes.default_canvas()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ws = ROOT.RooWorkspace(\"ws\")\n",
      "ws.factory(\"Exponential:e(x[0,500],tau[-0.01,-5,-0.001])\")       # exponential background PDF\n",
      "ws.factory(\"Gaussian::g(x,mean[250],sigma[15])\")    # Gaussian signal PDF\n",
      "ws.factory(\"SUM::model(b[1000,0,5000]*e,s[10,0,100]*g)\")          # composite model\n",
      "ws.saveSnapshot(\"initial_pars\",\"tau,mean,sigma,s,b\")             # make snapshot of parameter values\n",
      "%rp ws.Print()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This toy model depends on the variables $x,\\tau,\\mu,\\sigma,s,b$. The results of our measurement is $x$ and the parameter we are interested in is the number of observed signal events $s$. Therefore, the variables are categorised as follows:\n",
      "<ul>\n",
      "<li>the <i>observable</i> $x$,</li>\n",
      "<li>the <i>nuisance parameters</i> $\\tau, \\mu, \\sigma$ and $b$,</li>\n",
      "<li>and the <i>parameter of interest</i> $s$.</li>\n",
      "</ul><br />\n",
      "The likelihood function of the toy model is given by\n",
      "$$\\mathcal{L}(\\vec{x}|s,\\vec{\\theta}) = \\prod_{i=1}^n \\left[ s \\times \\text{G}(x_i|\\mu,\\sigma) + b \\times \\text{Exp}(x_i|\\tau) \\right] \\times \\text{Pois}(n|s+b)$$ where $\\vec{x}$ is the set of observations of $x$ and $\\vec{\\theta}$ denotes the set of nuisance parameters. It is handy to store this information for later usage into a <code>ModelConfig</code> object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set configuration\n",
      "mc = ROOT.RooStats.ModelConfig(ws)\n",
      "mc.SetPdf(\"model\") \n",
      "mc.SetObservables(\"x\")\n",
      "mc.SetNuisanceParameters(\"tau,mean,sigma,b\")\n",
      "mc.SetParametersOfInterest(\"s\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ws.loadSnapshot(\"initial_pars\")\n",
      "x = ws.var(\"x\")\n",
      "s = ws.var(\"s\")\n",
      "pdf = ws.pdf(\"model\")\n",
      "f = x.frame()\n",
      "d_obs = pdf.generate(ROOT.RooArgSet(x))  # generate some data which we will use as observation\n",
      "pdf.fitTo(d_obs)\n",
      "d_obs.plotOn(f)\n",
      "pdf.plotOn(f)\n",
      "f.Draw()\n",
      "c1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Significances and $p$-values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Much of the following is related to the field of <i>hypothesis testing</i> in which $p$-values play an important role. As a reminder, the definition of a $p$-value shall be repeated here:<br /><br />\n",
      "<i>\"Given some data $\\vec{x}$ and a hypothesis $H$, the $p$-value is defined as the probability, under the assumption of $H$ being true, of obtaining data with equal or worse compatibility than the actual observation.\"</i><br /><br />\n",
      "The following example illustrates this concept for a normal distributed observable for two hypothesis with different means and widths."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.stats import norm\n",
      "import numpy as np\n",
      "x = np.linspace(-6,6,1000)\n",
      "g1 = norm(loc=0,scale=1)\n",
      "g2 = norm(loc=3,scale=2)\n",
      "y1 = g1.pdf(x)\n",
      "y2 = g2.pdf(x)\n",
      "plt.plot(x,y1)\n",
      "plt.plot(x,y2)\n",
      "plt.plot([1.5,1.5],[0,0.3])\n",
      "plt.gca().fill_between(x,0,y1,where=x>1.5)\n",
      "plt.gca().fill_between(x,0,y2,where=x<1.5,color=\"green\")\n",
      "plt.text(2,0.1,\"p-value\",color=\"blue\",size=\"x-large\")\n",
      "plt.text(-1,0.15,\"p-value\",color=\"green\",size=\"x-large\")\n",
      "plt.text(1.6,0.25,\"x = 1.5\",color=\"red\",size=\"x-large\")\n",
      "plt.text(-4,0.35,\"H = G(0,1)\",color=\"blue\",size=\"x-large\")\n",
      "plt.text(-4,0.32,\"p = %.1f%%\" % (100*g1.sf(1.5)),color=\"blue\",size=\"x-large\")\n",
      "plt.text(2,0.35,\"H = G(3,2)\",color=\"green\",size=\"x-large\")\n",
      "plt.text(2,0.32,\"p = %.1f%%\" % (100*g2.cdf(1.5)),color=\"green\",size=\"x-large\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In everyday problems it is less obvious how <i>worse compatibility</i> is defined. In order to facilitate the comparison of two datasets with respect to their compatibility with a certain hypothesis, one needs to make use of so-called <i>test statistics</i>.<br />\n",
      "Papers usually cite significances rather than $p$-values as the latter can become very tiny. The significance is defined as the value $z$ such that the right-tail probability of a normal distribution above $z$ is equal to $p$.\n",
      "$$p = \\int_z^{+\\infty} \\text{G}(x,0,1) dx \\quad \\Rightarrow \\quad z = \\Phi^{-1}(1 - p)$$\n",
      "wher $\\Phi^{-1}$ is the inverse of the cumulative normal distribution. Significances and $p$-values can easily be converted using <br />\n",
      "<code>p = ROOT.Math.normal_cdf_c(z,1)</code><br />\n",
      "<code>z = ROOT.Math.normal_quantile_c(p,1)</code>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Test Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Measurements can be difficult to describe in a general approach as they can be very different (e.g. varying number of observables, discrete vs. continue observables etc). Therefore, the statistical data interpretation happens on a higher, more abstract level (well, this \"abstract\" level is going to be $\\mathbb{R}$ $\\dots$). You can think of a <i>test statistic</i> $t$ as a mapping from your space of observables to this abstract level: $t: \\text{measurement} \\mapsto \\mathbb{R}$. A test statistic should have the following properties:\n",
      "<ul>\n",
      "<li>It should depend on the hypothesis (e.g. the signal strength, position of the Gaussian etc).</li>\n",
      "<li>It should depend on the data observed.</li>\n",
      "<li>It should allow for a quantitative evaluation of the compatibility of the data with the given hypothesis.</li>\n",
      "</ul><br />\n",
      "A common (and very reasonable) choice is the profile likelihood ratio $\\lambda_s = \\frac{\\mathcal{L}(\\vec{x}|s,\\hat{\\hat{\\theta}})}{\\mathcal{L}(\\vec{x}|\\hat{s},\\hat{\\theta})}$ where $s$ is the parameter of interest and $\\theta$ is the set of nuisance parameters. Hereby, $\\hat{s}$ and $\\hat{\\theta}$ are the unconditional maximum likelihood estimators (MLE) which maximise the likelihood function globally. On the contrary, $\\hat{\\hat{\\theta}}$ is the conditional MLE which maximises the likelihood function for a given (and fixed) value of $s$. It is wortwhile to note that $\\lambda_s$ is for a given dataset $\\vec{x}$ only a function of $s$ and $0 \\le \\lambda_s \\le 1$. Values close to 1 correspond to good compatibility between the tested value of $s$ and the data while values close to 0 indicate a very poor agreement. The test statistic is then defined as\n",
      "$$t_s = -2 \\log \\lambda_s = -2 \\log \\frac{\\mathcal{L}(\\vec{x}|s,\\hat{\\hat{\\theta}})}{\\mathcal{L}(\\vec{x}|\\hat{s},\\hat{\\theta})} = 2 \\times \\left[ - \\log \\mathcal{L}(\\vec{x}|s,\\hat{\\hat{\\theta}}) - (- \\log \\mathcal{L}(\\vec{x}|\\hat{s},\\hat{\\theta})) \\right]\\ .$$\n",
      "The test statistics $t_s$ takes on values between 0 and $+\\infty$ with increasing values representing decreasing compatibility between the data and the tested hypothesis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "from matplotlib import cm\n",
      "x = np.arange(-3,3,0.1)\n",
      "y = np.arange(-3,3,0.1)\n",
      "xx,yy = np.meshgrid(x,y)\n",
      "z = 2*xx**2 + yy**2\n",
      "ax = plt.gca(projection='3d')\n",
      "ax.plot_surface(xx,yy,z,cmap=cm.gist_rainbow,rstride=1,cstride=1,linewidth=0)\n",
      "ax.plot(xs=len(y)*[-1.5],ys=y,zs=2*1.5**2 + y**2,color=\"red\",linewidth=3)\n",
      "ax.set_xlabel(\"x\")\n",
      "ax.set_ylabel(\"y\")\n",
      "ax.set_zlabel(\"L(x,y)\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Using sampling distributions to calculate $p$-values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After having chosen a test statistics, one can evaluate it for the hypothesis of interest on the measured dataset which yields $t_{s,\\mathrm{obs}}$ (= a single number). In order to interpret this single number (i.e. calculate a $p$-value) one needs to know the distribution of $t_s$. The so-called <i>sampling distribution</i> $f(t_s|s^\\prime)$ gives the probability of observing a value of the test statistic $t_s$ given that the true value of $s$ is equal to $s^\\prime$. In general, the tested hypothesis $s$ does not necessarily need to be the same as the assumed true hypothesis $s^\\prime$.<br />\n",
      "The code below generates the two sampling distributions $f(t_{s=30}|s^\\prime = 30)$ and $f(t_{s=0}|s^\\prime = 30)$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h_t0 = ROOT.TH1F(\"h_t0\",\"h_t0;t_{s};f(t_{s}|s')\",50,0,25)\n",
      "h_t30 = ROOT.TH1F(\"h_t30\",\"sampling distribution;t_{s};f(t_{s}|s^{#prime})\",50,0,25)\n",
      "ntoys = 2000\n",
      "x = ws.var(\"x\")\n",
      "s = ws.var(\"s\")\n",
      "pdf = ws.pdf(\"model\")\n",
      "for i in range(0,ntoys):\n",
      "    if i % int(0.01 * ntoys) == 0:\n",
      "        print \"\\r%.0f%% done\" % (i*100./ntoys),\n",
      "    ws.loadSnapshot(\"initial_pars\")\n",
      "    toy_data = pdf.generate(ROOT.RooArgSet(x))\n",
      "    r_uncond = pdf.fitTo(toy_data,ROOT.RooFit.Save(),ROOT.RooFit.PrintLevel(-1))\n",
      "    s.setConstant(True)\n",
      "    s.setVal(30)\n",
      "    r_cond30 = pdf.fitTo(toy_data,ROOT.RooFit.Save(),ROOT.RooFit.PrintLevel(-1))\n",
      "    s.setVal(0)\n",
      "    r_cond0 = pdf.fitTo(toy_data,ROOT.RooFit.Save(),ROOT.RooFit.PrintLevel(-1))\n",
      "    h_t0.Fill(2*(r_cond0.minNll() - r_uncond.minNll()))\n",
      "    h_t30.Fill(2*(r_cond30.minNll() - r_uncond.minNll()))\n",
      "    del toy_data\n",
      "    del r_uncond\n",
      "    del r_cond30\n",
      "    del r_cond0\n",
      "h_t30.Draw()\n",
      "h_t0.SetLineColor(ROOT.kRed)\n",
      "h_t0.Draw(\"same\")\n",
      "c1.SetLogy(1)\n",
      "l = ROOT.TLegend(0.6,0.6,0.85,0.85)\n",
      "l.SetFillColor(ROOT.kWhite)\n",
      "l.AddEntry(h_t30,\"f(t_{s=30}|s' = 30)\")\n",
      "l.AddEntry(h_t0,\"f(t_{s=0}|s' = 30)\")\n",
      "l.Draw(\"same\")\n",
      "c1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since larger values for $t_s$ indicate a worse agreement, the $p$-value can be calculated from the sampling distribution as\n",
      "$$p = \\int_{t_{s,obs}}^{+\\infty} f(t_s|s) d t_s$$\n",
      "Note: here $s = s^\\prime$ as the $p$-value is defined under the assumption that the tested hypothesis is true."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Expected and Observed Significances"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to state the significance of a measurement (e.g. the observation of some signal), one needs to specify the $p$-value for the alternative hypothesis (usually a background-only model). The smaller the $p$-value (= the larger the significance) the more unlikely the data can be explained the by the alternative hypothesis. A large significance <b>NEVER EVER</b> means that your signal has been discovered. It only means that the data can not be described by the alternative hypothesis. Due to this reason, an observed significance only depends on the data and the alternative hypothesis and never on the signal hypothesis. On the contrary, the expected significance, of course, depends on your signal model.<br />\n",
      "The test statistic for calculating significances is given by:\n",
      "$$q_0 = \\left\\{ \\begin{matrix} -2 \\log \\frac{\\mathcal{L}(\\vec{x}|s,\\hat{\\hat{\\theta}})}{\\mathcal{L}(\\vec{x}|\\hat{s},\\hat{\\theta})}, & \\text{for } \\hat{s} \\ge 0 \\\\ 0, & \\text{for } \\hat{s} < 0 \\end{matrix} \\right. $$\n",
      "This choice only considers an <i>excess</i> over the background model as indication for a signal while a deficit in data will not be regarded as hint of new phyiscs. This is appropriate in most situations but may fail in certain special cases (e.g. disappearance in neutrino oscillations).<br />\n",
      "The procedure for calculating the observed significance is the following:\n",
      "<ol>\n",
      "<li>Generate the sampling distribution $f(t_{s=0}|s'=0)$.</li>\n",
      "<li>Calculate $t_{s=0,\\mathrm{obs}}$ for the measured dataset.</li>\n",
      "<li>Calculate the $p$-value by integrating the sampling distribution.</li>\n",
      "</ol>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate sampling distribution\n",
      "h_t0 = ROOT.TH1F(\"h_t0\",\"h_t0\",50,0,25)\n",
      "ntoys = 1000\n",
      "s = ws.var(\"s\")\n",
      "s.setMin(-1)\n",
      "for i in range(0,ntoys):\n",
      "    if i % 10 == 0:\n",
      "        print \"\\r%.0f%% done\" % (i*100./ntoys),\n",
      "    ws.loadSnapshot(\"initial_pars\")\n",
      "    s.setVal(0)\n",
      "    d = pdf.generate(ROOT.RooArgSet(x))\n",
      "    r1 = pdf.fitTo(d,ROOT.RooFit.Save())\n",
      "    #%rp r1.Print(\"v\")\n",
      "    if s.getVal() < 0:\n",
      "        h_t0.Fill(0)\n",
      "    else:\n",
      "        s.setConstant(True)\n",
      "        s.setVal(0)\n",
      "        r2 = pdf.fitTo(d,ROOT.RooFit.Save())\n",
      "        #%rp r2.Print(\"v\")\n",
      "        h_t0.Fill(2*(r2.minNll() - r1.minNll()))\n",
      "        del r2\n",
      "    del d\n",
      "    del r1\n",
      "# calculate t_{s=0,obs}\n",
      "ws.loadSnapshot(\"initial_pars\")\n",
      "r1 = pdf.fitTo(d_obs,ROOT.RooFit.Save())\n",
      "#%rp r1.Print(\"v\")\n",
      "if s.getVal() < 0:\n",
      "    t0_obs = 0\n",
      "else:    \n",
      "    s.setConstant(True)\n",
      "    s.setVal(0)\n",
      "    r2 = pdf.fitTo(d_obs,ROOT.RooFit.Save())\n",
      "    #%rp r2.Print(\"v\")\n",
      "    t0_obs = 2*(r2.minNll() - r1.minNll())\n",
      "    \n",
      "# plot everything\n",
      "h_t0.Scale(1./h_t0.Integral(\"width\"))\n",
      "h_t0.Draw()\n",
      "# plot chi2 function\n",
      "f_chi2 = ROOT.TF1(\"chi2\",\"0.5*ROOT::Math::chisquared_pdf(x,1)\",0,25)\n",
      "f_chi2.SetLineColor(ROOT.kGreen+3)\n",
      "f_chi2.Draw(\"same\")\n",
      "# plot observation\n",
      "line = ROOT.TLine(t0_obs,0.001,t0_obs,1)\n",
      "line.Draw(\"same\")\n",
      "c1.SetLogy(1)\n",
      "c1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above test statistic is a smart choice as their asymptotic distribution is well-known. If certain requirements (e.g. enough statistics, not near parameter boundardies, no pathologies) are fulfilled, the sampling distribution is given by\n",
      "$$f(q_0|s'=0) = \\frac{1}{2}\\chi^2(q_0,\\text{ndf})$$\n",
      "where the number of degrees of freedom (ndf) is equal to the number of parameters of interest. In this case, the significance can easily be calculated as $z = \\sqrt{q_{0,\\mathrm{obs}}}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from math import sqrt\n",
      "# integrate sampling distribution\n",
      "bin = h_t0.GetXaxis().FindFixBin(t0_obs)\n",
      "p = sum(map(h_t0.GetBinContent,range(bin+1,h_t0.GetNbinsX()+2)))*h_t0.GetBinWidth(1)\n",
      "p += (t0_obs - h_t0.GetBinLowEdge(bin))*h_t0.GetBinContent(bin)\n",
      "print \"significance Z = %.2f/%.2f (toys,asymptotic)\" % (ROOT.Math.normal_quantile_c(p,1),sqrt(t0_obs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}